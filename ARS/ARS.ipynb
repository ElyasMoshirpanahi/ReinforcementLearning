{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e5a2e89",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e3b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "import pybullet_envs\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1fbef3",
   "metadata": {},
   "source": [
    "## Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703200d3",
   "metadata": {
    "code_folding": [
     1,
     14,
     34,
     57,
     106
    ]
   },
   "outputs": [],
   "source": [
    "#@title Hyperparameters class\n",
    "class hyper_parameters():\n",
    "    def __init__(self):\n",
    "        self.number_of_steps = 1000\n",
    "        self.episode_lenght = 1000\n",
    "        self.learning_rate = 0.02\n",
    "        self.number_of_directions = 16\n",
    "        self.number_of_best_directions = 16\n",
    "        assert self.number_of_best_directions <= self.number_of_directions\n",
    "        self.noise = 0.03\n",
    "        self.seed = 1\n",
    "        self.env_name = 'HalfCheetahBulletEnv-v0'\n",
    "\n",
    "#@title Normalizer class\n",
    "class Normalizer():\n",
    "    def __init__(self, number_of_inputs):\n",
    "        self.n = np.zeros(number_of_inputs)\n",
    "        self.mean = np.zeros(number_of_inputs)\n",
    "        self.mean_diff = np.zeros(number_of_inputs)\n",
    "        self.variance = np.zeros(number_of_inputs)\n",
    "\n",
    "    def observe(self, x):\n",
    "        self.n += 1.\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min=1e-2)\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        observed_mean = self.mean\n",
    "        observed_std = np.sqrt(self.var)\n",
    "        return (inputs - observed_mean) / observed_std\n",
    "\n",
    "#@title Policy class\n",
    "class Policy:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.theta = np.zeros((output_size, input_size))\n",
    "\n",
    "    def evaluate(self, input, delta=None, direction=None):\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input)\n",
    "        elif direction == \"positive\":\n",
    "            return (self.theta + hp.noise * delta).dot(input)\n",
    "        else:\n",
    "            return (self.theta - hp.noise * delta).dot(input)\n",
    "\n",
    "    def sample_deltas(self):\n",
    "        return [np.random.randn(*self.theta.shape) for i in range(hp.number_of_directions)]\n",
    "\n",
    "    def update(self, rollouts, sigma_r):\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_positive, r_negative, d in rollouts:\n",
    "            step += (r_positive - r_negative) * d\n",
    "\n",
    "        self.theta += hp.learning_rate / (hp.number_of_best_directions * sigma_r) * step               \n",
    "        \n",
    "#@title Exploring the policy on one sepcific direction and over one episode\n",
    "def explore(env,normalizer, policy, direction = None , delta =None):\n",
    "    state = env.reset()\n",
    "    done  = False\n",
    "    num_plays = 0.\n",
    "    sum_rewards = 0.\n",
    "    \n",
    "    while not done and num_plays < hp.episode_lenght:\n",
    "        normalizer.observe(state)\n",
    "        state  = normalizer.normalize(state)\n",
    "        action = policy.evaluate(state , delta ,direction)\n",
    "        state , reward , done, _ =env.step(action)\n",
    "        reward  = max(min(reward,1), -1 )#Important to get the outliers\n",
    "        sum_rewards  += reward\n",
    "        num_plays  +=1\n",
    "    return sum_rewards\n",
    "\n",
    "#@title Training AI\n",
    "def train(env, policy, normalizer, hp):\n",
    "    for step in range(hp.number_of_steps):\n",
    "        start = dt.now()\n",
    "        # Initializing the perturbation deltas and positive/negative rewards\n",
    "        deltas = policy.sample_deltas()\n",
    "        positive_rewards = [0] * hp.number_of_directions\n",
    "        negative_rewards = [0] * hp.number_of_directions\n",
    "        \n",
    "        # Getting the positive rewards in the positive directions\n",
    "        for k in range(hp.number_of_directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy, direction=\"positive\", delta=deltas[k])\n",
    "          \n",
    "        # Getting the negative rewards in the negative directions\n",
    "        for k in range(hp.number_of_directions):\n",
    "            negative_rewards[k] = explore(env, normalizer, policy, direction=\"negative\", delta=deltas[k])\n",
    "        \n",
    "        # Gathering all rewards to compute the standard deviation of rewards\n",
    "        all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "        sigma_r = all_rewards.std()\n",
    "        \n",
    "        # Scoring the rollouts by max(r_pos, r_neg) and selecting the best directions\n",
    "        scores = {k: max(r_pos, r_neg) for k, (r_pos, r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "        order = sorted(scores.keys(), key=lambda x: scores[x])[:hp.number_of_best_directions]\n",
    "        rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "        \n",
    "        # Updating the policy\n",
    "        policy.update(rollouts, sigma_r)\n",
    "        \n",
    "        # Printing the final reward of the policy after the update\n",
    "        reward_evaluation = explore(env, normalizer, policy)\n",
    "        end =  dt.now() - start \n",
    "        print(f\"Step: {step} Reward: {reward_evaluation} Time: {end}\")\n",
    "\n",
    "#@title Making Dirs\n",
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934a723",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583b93cd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#@title Iniating the parameter and running the model\n",
    "working_dir = mkdir('./exp', 'brs')\n",
    "monitor_dir = mkdir(working_dir, 'monitor')\n",
    "\n",
    "hp = hyper_parameters()\n",
    "np.random.seed(hp.seed)\n",
    "env = gym.make(hp.env_name)\n",
    "env = wrappers.Monitor(env, monitor_dir, force=True)\n",
    "nb_inputs = env.observation_space.shape[0]\n",
    "nb_outputs = env.action_space.shape[0]\n",
    "policy = Policy(input_size=nb_inputs, output_size=nb_outputs)\n",
    "normalizer = Normalizer(nb_inputs)\n",
    "train(env, policy, normalizer, hp)\n",
    "\n",
    "\n",
    "#to check the output check the exp/brs and monitor folders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
